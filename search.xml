<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>流系统漫游（持续更新中...）</title>
    <url>/2020/06/27/%E6%B5%81%E7%B3%BB%E7%BB%9F%E6%BC%AB%E6%B8%B8/</url>
    <content><![CDATA[<p>本文从谷歌Dataflow system论文的角度初探streaming system。谷歌从GFS、MapReduce开始，到目前的分布式操作系统Borg和流系统，大部分的论文都是从描述现实的应用场景出发，然后提出解决方案。其本质是需求驱动技术的发展。目前大部分的科研是：提出一种方法，解决一个问题，方法的效果。与单纯的科研驱动发展不同比如数理化，单纯的研究最开始时是没有实用价值的，但后来才被自动化机器学习等专业应用后证明其应用潜力。</p>
<h2 id="1-应用场景"><a href="#1-应用场景" class="headerlink" title="1. 应用场景"></a>1. 应用场景</h2><p>一个流媒体视频提供商(Google)想要通过显示视频广告和向广告客户(Apple)收取观看广告的费用来实现广告变现。</p>
<p>该平台(YOutube)支持内容和广告的在线和离线查看。视频提供商想知道每天每个广告客户需要支付多少费用，并收集有关视频和广告的统计数据。此外，他们还想对大量的历史数据进行有效的离线实验。</p>
<p>广告商/内容提供商想要知道他们的视频被观看的频率和时间，观看的内容/广告是什么，观看的人群是什么。他们也想知道他们要付多少钱。他们希望尽可能快地获得所有这些信息，这样他们就可以调整预算和投标，改变目标，调整活动，并尽可能实时地规划未来的方向。因为涉及到钱，所以正确性是最重要的。</p>
<p>虽然数据处理系统本质上是复杂的，但是视频提供商需要一个简单而灵活的编程模型。最后，由于互联网极大地扩展了任何可以沿着其主干分布的业务的范围，它们还需要一个能够处理全球范围内散居的数据的系统。</p>
<p>对于这样的用例，必须计算的信息本质上是每个视频观看的时间和长度、谁观看了它，以及它与哪个广告或内容配对(即每个用户、每个视频观看会话)。从概念上讲，这很简单，但是现有的模型和系统都不能满足规定的需求。</p>
<h2 id="2-平台"><a href="#2-平台" class="headerlink" title="2. 平台"></a>2. 平台</h2><p>Flink是一个流系统，在德语中， Flink 一词表示快速和灵巧，项目采用一只松鼠的彩色图案作为 logo，<br>这不仅是因为松鼠具有快速和灵巧的特点，还因为柏林的松鼠有一种迷人的红棕色，<br>而 Flink 的松鼠 logo 拥有可爱的尾巴，尾巴的颜色与 Apache 软件基金会的 logo 颜<br>色相呼应，也就是说，这是一只 Apache 风格的松鼠。</p>
<p>Flink 项目的理念是：“ Apache Flink 是为分布式、高性能、随时可用以及准确<br>的流处理应用程序打造的开源流处理框架”。<br>Apache Flink 是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。 Flink 被设计在所有常见的集群环境中运行，以内存执行速度和任意规模<br>来执行分布式计算。</p>
<h3 id="3-Flink架构"><a href="#3-Flink架构" class="headerlink" title="3. Flink架构"></a>3. Flink架构</h3><p><img src="https://img-blog.csdnimg.cn/20200318173107636.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI4MjU4OTAz,size_16,color_FFFFFF,t_70" alt="image"></p>
<h3 id="4-Flink-特点"><a href="#4-Flink-特点" class="headerlink" title="4. Flink 特点"></a>4. Flink 特点</h3><h4 id="4-1-事件驱动型-Event-driven"><a href="#4-1-事件驱动型-Event-driven" class="headerlink" title="4.1 事件驱动型(Event-driven)"></a>4.1 事件驱动型(Event-driven)</h4><p>应用从一个或多个事件流提取数据，并<br>根据到来的事件触发计算、状态更新或其他外部动作。比较典型的就是以 kafka 为<br>代表的消息队列几乎都是事件驱动型应用（但是与Sparkstreaming不同）。</p>
<p><img src="https://img-blog.csdnimg.cn/20200318173621659.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI4MjU4OTAz,size_16,color_FFFFFF,t_70" alt="image"></p>
<h4 id="4-2-流与批的世界观"><a href="#4-2-流与批的世界观" class="headerlink" title="4.2 流与批的世界观"></a>4.2 流与批的世界观</h4><ul>
<li><b>批处理</b>的特点是有界、持久、大量， 非常适合需要访问全套记录才能完成的计<br>算工作，一般用于离线统计。</li>
<li><b>流处理</b>的特点是无界、实时, 无需针对整个数据集执行操作，而是对通过系统<br>传输的每个数据项执行操作，一般用于实时统计。</li>
</ul>
<p>在 spark 的世界观中，一切都是由批次组成的，离线数据是一个大批次，而实<br>时数据是由一个一个无限的小批次组成的。</p>
<p>而在 flink 的世界观中，一切都是由流组成的，离线数据是有界限的流，实时数<br>据是一个没有界限的流，这就是所谓的有界流和无界流。</p>
<h3 id="5-Flinkj架构"><a href="#5-Flinkj架构" class="headerlink" title="5. Flinkj架构"></a>5. Flinkj架构</h3><p>只要由作业管理器（JobManager）、资源管理器（ResourceManager）、任务管理器（TaskManager），<br>以及分发器（Dispatcher）组成。</p>
<h4 id="JogManager"><a href="#JogManager" class="headerlink" title="JogManager"></a>JogManager</h4><p>控制应用程序执行的主进程，每个应用程序都会被一个不同的<br>JobManager 所控制执行。 JobManager 会先接收到要执行的应用程序， 这个应用程序会包括：<br>作业图（JobGraph）、逻辑数据流图（logical dataflow graph）和打包了所有的类、库和其它<br>资源的 JAR 包。 JobManager 会把 JobGraph 转换成一个物理层面的数据流图，这个图被叫做<br>“执行图”（ExecutionGraph），包含了所有可以并发执行的任务。 JobManager 会向资源管<br>理器（ResourceManager）请求执行任务必要的资源，也就是任务管理器（TaskManager）上<br>的插槽（ slot）。一旦它获取到了足够的资源，就会将执行图分发到真正运行它们的<br>TaskManager 上。而在运行过程中， JobManager 会负责所有需要中央协调的操作，比如说检<br>查点（checkpoints）的协调。</p>
<h4 id="ResourceManager"><a href="#ResourceManager" class="headerlink" title="ResourceManager"></a>ResourceManager</h4><p>主要负责管理任务管理器（TaskManager）的插槽（slot）， TaskManger 插槽是 Flink 中<br>定义的处理资源单元。 Flink 为不同的环境和资源管理工具提供了不同资源管理器，比如<br>YARN、 Mesos、 K8s，以及 standalone 部署。当 JobManager 申请插槽资源时， ResourceManager<br>会将有空闲插槽的 TaskManager 分配给 JobManager。如果 ResourceManager 没有足够的插槽<br>来满足 JobManager 的请求，它还可以向资源提供平台发起会话，以提供启动 TaskManager<br>进程的容器。另外， ResourceManager 还负责终止空闲的 TaskManager，释放计算资源。</p>
<h4 id="TaskManager"><a href="#TaskManager" class="headerlink" title="TaskManager"></a>TaskManager</h4><p>通常在 Flink 中会有多个 TaskManager 运行，每一个 TaskManager<br>都包含了一定数量的插槽（slots）。插槽的数量限制了 TaskManager 能够执行的任务数量。<br>启动之后， TaskManager 会向资源管理器注册它的插槽；收到资源管理器的指令后，<br>TaskManager 就会将一个或者多个插槽提供给 JobManager 调用。 JobManager 就可以向插槽<br>分配任务（tasks）来执行了。在执行过程中，一个 TaskManager 可以跟其它运行同一应用程<br>序的 TaskManager 交换数据。</p>
<h2 id="else"><a href="#else" class="headerlink" title="else"></a>else</h2><h3 id="flink-内存管理"><a href="#flink-内存管理" class="headerlink" title="flink 内存管理"></a>flink 内存管理</h3><ul>
<li><a href="https://zhuanlan.zhihu.com/p/27241485" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/27241485</a></li>
</ul>
<h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><h3 id="flink安装"><a href="#flink安装" class="headerlink" title="flink安装"></a>flink安装</h3><ul>
<li><a href="https://blog.csdn.net/zhanaolu4821/article/details/86001144" target="_blank" rel="noopener">dokcer安装</a></li>
</ul>
]]></content>
      <categories>
        <category>distributed system</category>
      </categories>
      <tags>
        <tag>distribute</tag>
        <tag>big date</tag>
        <tag>streaming system</tag>
      </tags>
  </entry>
  <entry>
    <title>未来数据库趋势-Tidb</title>
    <url>/2020/06/27/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9C%AA%E6%9D%A5%E8%B6%8B%E5%8A%BF/</url>
    <content><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><ul>
<li><p>未来数据库趋势：大一统，就像《魔戒》中说的 ：ONE RING TO RULE THEM ALL，就是一套解决方案去解决扩展性，ACID，高性能，稳定性，大数据，机器学习计算能力（HATP+分布式）。</p>
</li>
<li><p>目前大多数的科研论文陷入跑分思维，在一个特定的 Workload下，然后把 Oracle 摁在地上摩擦，这样的论文有很多。但是大家回头看看 Oracle 还是王者。</p>
</li>
<li><p>紧跟现实应用场景（这是典型的工程思维，科研应该在理论上有所突破）</p>
</li>
</ul>
<h2 id="未来发展的基石"><a href="#未来发展的基石" class="headerlink" title="未来发展的基石"></a>未来发展的基石</h2><ul>
<li>硬件的发展：SDD，多核CPU，万兆网卡，虚拟化。</li>
<li>数据库这个行业里面很多的假设，在现在新的硬件的环境下其实都是不成立的。</li>
<li>为什么 B-Tree 就一定会比 LSM-Tree 要快呢？不一定，我跑到 Flash 或者 NVMe SSD 、Optane 甚至未来的持久化内存这种介质上，那数据结构设计完全就发生变化了。过去可能需要投入很多精力去做的数据结构，现在可以采用暴力算法。</li>
<li>分布式理论的发展，如raft</li>
</ul>
<h2 id="未来趋势"><a href="#未来趋势" class="headerlink" title="未来趋势"></a>未来趋势</h2><h3 id="1-log-is-the-new-database"><a href="#1-log-is-the-new-database" class="headerlink" title="1. log is the new database"></a>1. log is the new database</h3><ul>
<li>Hyper实验中正常的 SQL 语句的执行时间，比如说直接把一语句放到另外一个库里去执行，耗时最多。逻辑日志存放时，耗时大概能快 23%，存放物理日志时能快 56%。所以TiDB 里的 TiFlash 其实同步的是 Raft 日志，而并不是同步 Binlog 或者其他。</li>
<li>Aurora同步的是 redo log 。其实他的好处也很明显，也比较直白，就是 I/O 更小，网络传输的 size 也更小，所以就更快。</li>
<li>多raft 组<h3 id="2-Vectorized"><a href="#2-Vectorized" class="headerlink" title="2. Vectorized"></a>2. Vectorized</h3></li>
</ul>
<ul>
<li>TiDB SQL 引擎用 Volcano 模型，这个模型遍历一棵物理计划的树，不停的调 Next，每一次 Next 都是调用他的子节点的 Next，然后再返回结果。这个模型有几个问题：第一是每一次都是拿一行，导致 CPU 的 L1、L2 缓存利用率很差，没有办法利用多 CPU 的 Cache。第二，在真正实现的时候，它内部的架构是一个多级的虚函数调用。大家知道虚函数调用在 Runtime 本身的开销是很大的，在《MonetDB/X100: Hyper-Pipelining Query Execution》（<a href="http://cidrdb.org/cidr2005/papers/P19.pdf）" target="_blank" rel="noopener">http://cidrdb.org/cidr2005/papers/P19.pdf）</a> 里面提到，在跑 TPC-H 的时候，Volcano 模型在 MySQL 上跑，大概有 90% 的时间是花在 MySQL 本身的 Runtime 上，而不是真正的数据扫描。所以这就是 Volcano 模型一个比较大的问题。第三，如果使用一个纯静态的列存的数据结构，大家知道列存特别大问题就是它的更新是比较麻烦的， 至少过去在 TiFlash 之前，没有一个列存数据库能够支持做增删改查。那在这种情况下，怎么保证数据的新鲜？这些都是问题。</li>
<li>TiDB SQL 引擎的 Volcano 模型，已经从一行一行变成了一个 Chunk 一个 Chunk，每个 Chunk 里面是一个批量的数据，所以聚合的效率会更高。</li>
<li>TiDB 中算子推到 TiKV 来做， TiKV会成为一个全向量化的执行器的框架。</li>
</ul>
<h3 id="3-Workload-Isolation"><a href="#3-Workload-Isolation" class="headerlink" title="3. Workload Isolation"></a>3. Workload Isolation</h3><ul>
<li>尽可能地把 OLTP 跟 OLAP 隔离开。</li>
<li>Google Spanner做le一个新的数据结构，来替代 Bigtable-Like SSTable 数据结构，这个数据结构叫 Ressi《Spanner: Becoming a SQL System》。其实表面上看还是行存，但内部也是一个 Chunk 变成列存这样的一个结构。</li>
<li>但即使是换一个新的数据结构，也没有办法很好做隔离，因为毕竟还是在一台机器上，在同一个物理资源上。最彻底的隔离是物理隔离。</li>
</ul>
<p>TiFlash 用了好几种技术来去保证数据是更新的。一是增加了 Raft Leaner，二是把 TiDB 的 MVCC 也实现在了 TiFlash 的内部。第三在 TiFlash 接触了更新（的过程），在 TiFlash 内部还有一个小的 Memstore，来处理更新的热数据结果，最后查询的时候，是列存跟内存里的行存去 merge 并得到最终的结果。</p>
<ul>
<li>TiFlash 的核心思想就是通过 Raft 的副本来做物理隔离。这个有什么好处呢？这是我们今天给出的答案，但是背后的思考，到底是什么原因呢？为什么我们不能直接去同步一个 binlog 到另外一个 dedicate 的新集群上（比如 TiFlash 集群），而一定要走 Raft log？最核心的原因是，我们认为 Raft log 的同步可以水平扩展的。因为 TiDB 内部是 Mult-Raft 架构，Raft log 是发生在每一个 TiKV 节点的同步上。</li>
<li>大家想象一下，如果中间是通过 Kafka 沟通两边的存储引擎，那么实时的同步会受制于中间管道的吞吐。比如一部分一直在更新，另一边并发写入每秒两百万，但是中间的 Kafka 集群可能只能承载 100 万的写入，那么就会导致中间的 log 堆积，而且下游的消费也是不可控的。而通过 Raft 同步， Throughput 可以根据实际存储节点的集群大小，能够线性增长。这是一个特别核心的好处。</li>
</ul>
<h3 id="4-SIMD"><a href="#4-SIMD" class="headerlink" title="4. SIMD"></a>4. SIMD</h3><ul>
<li>现代的 CPU 会支持一些批量的指令，比如像 _mm_add_epi32，可以一次通过一个 32 位字长对齐的命令，批量的操作 4 个累加。看上去只是省了几个 CPU 的指令，但如果是在一个大数据量的情况下，基本上能得到 4 倍速度的提升。</li>
<li>I/O不是瓶颈，未来的瓶颈在于CPU</li>
<li>怎么去用新的硬件，去尽可能的把计算效率提升，这是未来数据库发展的重点。比如说我怎么在数据库里 leverage GPU 的计算能力，因为如果 GPU 用的好，其实可以很大程度上减少计算的开销。</li>
<li>所以，如果在单机 I/O 这些都不是问题的话，下一个最大问题就是怎么做好分布式，这也是为什么Tidb一开始就选择了一条看上去更加困难的路：做一个 Share-nothing 的数据库，并不是像 Aurora 底下共享一个存储。</li>
</ul>
<h3 id="5-Dynamic-Data-placement"><a href="#5-Dynamic-Data-placement" class="headerlink" title="5. Dynamic Data placement"></a>5. Dynamic Data placement</h3><ul>
<li><p>今天其实看不到未来十年数据增长是怎样的，十年前大家不能想到现在我们的数据量有这么大。</p>
</li>
<li><p>所以新的架构或者新的数据库，一定要去面向我们未知的 Scale 做设计。比如大家想象现在有业务 100T 的数据，目前看可能还挺大的，但是有没有办法设计一套方案去解决 1P、2P 这样数据量的架构？</p>
</li>
<li><p>在海量的数据量下，怎么把数据很灵活的分片是一个很大的学问。</p>
</li>
<li><p>分库分表的 Router 是静态的。如果出现分片不均衡，比如业务可能按照 User ID 分表，但是发现某一地方 / 某一部分的 User ID 特别多，导致数据不均衡。</p>
</li>
<li><p>TiDB 彻底把分片从数据库里隔离了出来，放到了另外一个模块里。分片应该是根据业务的负载、根据数据的实时运行状态，来决定这个数据应该放在哪儿。这是传统的静态分片不能相比的，不管传统的用一致性哈希，还是用最简单的对机器数取模的方式去分片（都是不能比的）。</p>
</li>
<li><p>在这个架构下，甚至未来我们还能让 AI 来帮忙。把分片操作放到 PD 里面，它就像一个 DBA 一样，甚至预测 Workload 给出数据分布操作。比如课程报名数据库系统，系统发现可能明天会是报名高峰，就事先把数据给切分好，放到更好的机器上。这在传统方案下是都需要人肉操作，其实这些事情都应该是自动化的。</p>
</li>
<li><p>Dynamic Data placement 好处首先是让事情变得更 flexible ，对业务能实时感知和响应。另外还有一点，为什么我们有了 Dynamic Placement 的策略，还要去做 Table Partition。Table Partition相当于业务已经告诉我们数据应该怎么分片比较好，我们还可以做更多针对性的优化。这个 Partition 指的是逻辑上的 Partition ，是可能根据你的业务相关的，比如说一张表存着 2018 年的数据，虽然还是 TiDB 通过 PD 去调度，但是我知道你 Drop 这个 Table 的时候，一定是 Drop 这些数据，所以这样会更好，而且更加符合用户的直觉。但这样架构仍然有比较大的挑战。当然这个挑战在静态分片的模型上也都会有。</p>
</li>
<li><p>比如说围绕着这个问题，怎么更快的发现数据的热点，比如说我们的调度器，如果最好能做到，比如突然来个秒杀业务，我们马上就发现了，就赶紧把这块数据挪到好的机器上，或者把这块数据赶紧添加副本，再或者把它放到内存的存储引擎里。这个事情应该是由数据库本身去做的。所以为什么我们这么期待 AI 技术能够帮我们，是因为虽然在 TiDB 内部，用了很多规则和方法来去做这个事情，但人工的规则不是万能的。</p>
</li>
</ul>
<h3 id="6-Storage-and-Computing-Seperation"><a href="#6-Storage-and-Computing-Seperation" class="headerlink" title="6. Storage and Computing Seperation"></a>6. Storage and Computing Seperation</h3><p>说存储计算分离本质：存储依赖的物理资源，跟计算所依赖的物理资源并不一样。</p>
<ul>
<li>比如计算可能需要很多 CPU，需要很多内存来去做聚合，存储节点可能需要很多的磁盘和 I/O，如果全都放在一个组件里 ，调度器就会很难受：我到底要把这个节点作为存储节点还是计算节点？</li>
<li>在这块，可以让调度器根据不同的机型（来做决定），是计算型机型就放计算节点，是存储型机型就放存储节点。</li>
</ul>
<h3 id="7-Everything-is-Pluggable"><a href="#7-Everything-is-Pluggable" class="headerlink" title="7. Everything is Pluggable"></a>7. Everything is Pluggable</h3><ul>
<li>每一层我们未来都会去对外暴露一个非常抽象的接口，能够去 leverage 不同的系统的好处。 F1 Query 这篇论文，基本表述了一个大规模的分布式系统的期待，架构的切分非常漂亮。</li>
</ul>
<h3 id="8-Distributed-Transaction"><a href="#8-Distributed-Transaction" class="headerlink" title="8. Distributed Transaction"></a>8. Distributed Transaction</h3><ul>
<li>ACID 事务肯定是必要的，除了 Google 用了原子钟，Truetime 非常牛。</li>
<li>当然，时间戳，不管是用硬件还是软件分配，仍然是现今的最好方法。 </li>
<li>因为如果要摆脱中心事务管理器，时间戳还是很重要的。所以在这方面的挑战就会变成：怎么去减少两阶段提交带来的网络的 round-trips？或者如果有一个时钟的 PD 服务，怎么能尽可能的少去拿时间戳？</li>
<li>Tidb把 Percolator 模型做了一些优化，能够在数学上证明，可以少拿一次时钟数学证明的过程已经开源,用TLA+ 数学工具做了证明（ <a href="https://github.com/pingcap/tla-plus/blob/master/OptimizedCommitTS/OptimizedCommitTS.tla）。" target="_blank" rel="noopener">https://github.com/pingcap/tla-plus/blob/master/OptimizedCommitTS/OptimizedCommitTS.tla）。</a></li>
<li>Follower Read。很多场景读多写少，读的业务压力很多时候是要比写大很多的，Follower Read 能够帮我们线性扩展读的性能，而且在我们的模型上，因为==没有时间戳== ，所以能够在一些特定情况下保证不会去牺牲一致性。</li>
</ul>
<h3 id="9-Cloud-Native-Architecture"><a href="#9-Cloud-Native-Architecture" class="headerlink" title="9. Cloud-Native Architecture"></a>9. Cloud-Native Architecture</h3><ul>
<li>多租户没有做在 TiDB 的系统内部，因为其设计理念是「数据库就是数据库」，它并不是一个操作系统，不是一个容器管理平台。</li>
<li>模块和结构化是更清晰一种==软件工程==的方式。</li>
<li>Kubernetes 在这块已经做的足够好了，未来 K8s 会变成集群的新操作系统，会变成一个 Linux。比如说如果单机时代做一个数据库，会在数据库里面内置一个操作系统吗？肯定不会。</li>
<li>所以在模块抽象的边界，可以依赖 K8s 。《Large-scale cluster management at Google with Borg》这篇论文里面提到了一句话，BigTable 其实也跑在 Borg 上。</li>
</ul>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/86079949" target="_blank" rel="noopener">未来数据库应使用哪些技术？</a></li>
</ul>
]]></content>
      <categories>
        <category>DB</category>
        <category>survy</category>
      </categories>
      <tags>
        <tag>hatp</tag>
        <tag>tidb</tag>
      </tags>
  </entry>
  <entry>
    <title>docker安装Spark</title>
    <url>/2020/06/27/docker%20%E5%AE%89%E8%A3%85spark/</url>
    <content><![CDATA[<p>疫情期间身边只有笔记本，想学习一哈spark（因为正在研究declarative  ml，要看SystemML的论文），配置环境太麻烦，因此想用docker pull 一下网上现成的镜像，由于是私人hub，docker直接pull时还是网络慢到让人崩溃。。。采用github上的自己build，由于国内网络原因，安装了n次，好几个g的镜像，简直让人崩溃。</p>
<p>过了n天后，偶然想起阿里云提供了免费的doker 仓库并支在线build。真是爽歪歪。。。</p>
<p>一句话总结整个过程的安装方式：利用github上的Dockerfile，使用阿里云提供的容器镜像服务，构建对应的 Docker镜像，从阿里云中pull到本地。</p>
<p>从dockerfile 到 build 全部在线完成，没有网络问题，速度也杠杠的，不得不感谢阿里云云计算的NB(因为免费，所以感谢，所以NB，haha)。</p>
<p>现在记录一下过程：</p>
<ul>
<li>利用github fork <a href="https://github.com/sequenceiq/docker-spark。" target="_blank" rel="noopener">https://github.com/sequenceiq/docker-spark。</a></li>
<li>阿里云容器镜像服务关联上述仓库，然后build，如果可以build成功，ok，本文结束。</li>
</ul>
<p>人生之不如意，十有八九。下面记载我直接build过程会遇到如下错误（如果用github+阿里云，都会遇到下面的问题）：</p>
<p><img src="https://img-blog.csdnimg.cn/20200420223722488.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI4MjU4OTAz,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>找不到epel安装源，大概是/etc/yum.repos.d/epel.repo 中镜像列表是https的问题，所以我直接修正并新建了一个epel.repo文件，build时替换掉原先的文件。</p>
<ul>
<li><p>此外按照原先的rpm安装epel方式在安装R语言时会遇到依赖问题，因此我直接把Dockerfile中的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">RUN rpm -ivh http:&#x2F;&#x2F;dl.fedoraproject.org&#x2F;pub&#x2F;epel&#x2F;6&#x2F;x86_64&#x2F;epel-release-6-8.noarch.rpm</span><br></pre></td></tr></table></figure>
<p>修改为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">RUN yum -y install epel-release</span><br></pre></td></tr></table></figure>
<p>因为镜像在build的过程中不能和命令交互，因此yum时别忘了参数 ==-y==。</p>
</li>
<li><p>因为镜像太大，阿里云build时成功率不会是百分之百，比如：<br><img src="https://img-blog.csdnimg.cn/20200420230859514.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI4MjU4OTAz,size_16,color_FFFFFF,t_70" alt="image"></p>
</li>
</ul>
<p>具体原因是：</p>
<p><img src="https://img-blog.csdnimg.cn/20200420230927347.png" alt="image"></p>
<p>所以多尝试几次就好了（毕竟免费，免费）。。。</p>
<p>修改完后可以直接build成功的github仓库可以直接fork：<a href="https://github.com/kisisjrlly/docker-spark/" target="_blank" rel="noopener">https://github.com/kisisjrlly/docker-spark/</a></p>
<p>然后在本机上可以直接pull阿里云上build好的镜像啦，然后就可以愉快的玩耍啦。。。</p>
<h3 id="ref"><a href="#ref" class="headerlink" title="ref"></a>ref</h3><ul>
<li><a href="https://blog.csdn.net/weixin_43569697/article/details/89279225" target="_blank" rel="noopener">修改阿里下载镜像</a></li>
<li><a href="https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors" target="_blank" rel="noopener">阿里容器镜像服务</a></li>
<li><a href="https://mp.weixin.qq.com/s/kf0SrktAze3bT7LcIveDYw" target="_blank" rel="noopener">国内下载被墙的Docker镜像</a></li>
</ul>
]]></content>
      <categories>
        <category>experience</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark</title>
    <url>/2020/02/27/spark/</url>
    <content><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>spark：hadoop的升级版，通过定义新的数据抽象解决现有的问题，类似于数据结构中有数组，高维数组，集合，map，但是现有的数据结构不适用于特定的问题，spark抛开能想到的基础数据结构，重新定义新的数据抽象解决了现有以Hadoop为基础的大数据平台的特定问题。</p>
<ul>
<li>现有的大数据平台的缺点：<ul>
<li>Hadoop：保存到中间文件写磁盘，且不能重复利用中间数据，较多的磁盘IO等overead。</li>
<li>存在对hadoop的改进版本系统：Pregel，然而，这些框架只支持特定的计算模式(例如，循环一系列MapReduce步骤)，并隐式地为这些模式执行数据共享。它们不提供更一般重用的抽象，例如，让用户将多个数据集加载到内存中，并在其中运行特别的查询。</li>
</ul>
</li>
<li>spark的解决方法<ul>
<li>定义数据抽象RDD：具有容错，数据并行执行，数据可显示保存到内存中去，优化的数据存放策略和丰富的API的特性。</li>
<li>设计RDD主要的挑战是如何有效的容错（Google 的GFS，MapReduce 本质上是解决FT问题，因此引领大数据技术的发展）。<ul>
<li>现有的基于传统数据库的容错方式：replication+log overhead仍然较高，RDD可以通过重新计算（Lineage）的方式恢复（本质上的方法类似造火箭：我知道你是如何造出来的，你坏掉了我重新再造一个出来就好了）。</li>
<li>但是Lineage计算较长时还是会用到log。</li>
<li>Spark设计目的专注于批处理分析的高效编程模型，不适用于对共享变量进行异步细粒度更新的应用程序，比如分布式InMemoryDB，对于这些系统的工作还是交给RamCloud好了。。。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h3><p>是一种多范式的编程语言，类似于java，python编程，设计初衷是要集成面向对象编程和函数式编程的各种特性。</p>
<h3 id="什么是RDD"><a href="#什么是RDD" class="headerlink" title="什么是RDD"></a>什么是RDD</h3><p>RDD是一种数据抽象：弹性分布式数据集合（Resilient Distributed Dataset），是spark的基本元素，不可变，可分区，里面的元素可以被分布式并行执行（对码农透明）。</p>
<ul>
<li>Resilient<ul>
<li>代表数据可以存在内存也可以存到磁盘，计算快</li>
</ul>
</li>
<li>Distributed<ul>
<li>一个RDD被分布式存储，容错，又可以分布式并行计算</li>
</ul>
</li>
<li>Dataset<ul>
<li>类似于Hadoop中的文件，是一种抽象的分布式数据集合</li>
</ul>
</li>
</ul>
<h3 id="RDD五大特性"><a href="#RDD五大特性" class="headerlink" title="RDD五大特性"></a>RDD五大特性</h3><p>具有以下性质：</p>
<ul>
<li>A list of partitions<ul>
<li>一个RDD有多个分区，是一组分区列表，spark的task是以分区为单位，每个分布对应一个task线程（并行计算）。运行再worker节点的executor线程中。</li>
</ul>
</li>
<li>A function for computing each split<ul>
<li>函数会同时作用在所有的分区上。</li>
</ul>
</li>
<li>A list of dependencies on other RDDs<ul>
<li>新产生的RDD依赖于前期的存在的RDD（RDD被保存在内存中，可以被重复使用；可以实现无checkpoint+log的Fault tolerance机制）</li>
</ul>
</li>
<li>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)<ul>
<li>（可选项）对于KV类型的RDD可以hashparition存储（必须产生shffule），如果不是KV类型—-就表示木有</li>
<li>在spark中，有两种分区函数：<ul>
<li>第一种：HashPationer函数             对key去hashcode，然后对分区数取余得到对应的分区号———-&gt; key.hashcode % 分区数 = 分区号 。</li>
<li>第二种：RangeParitoner函数，按照一定的范围进行分区，相同范围的key会进入同一个分区。（A-H）—-&gt; 1号分区，（I-Z）—-&gt; 2号分区。</li>
</ul>
</li>
<li>Optionally, a list of preferred locations to compute each split on (e.g. block locations for<br>an HDFS file)<ul>
<li>（可选项）  一组最有的数据库位置列表：数据的本地性，数据的位置最优<ul>
<li>spark后期任务计算优先考虑存在数据的节点开启计算任务，也就是说数据在哪里，就在哪里开启计算任务，大大减少网络传输。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>以单词划分为例：<br><img src="https://img-blog.csdnimg.cn/20200227120058838.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI4MjU4OTAz,size_16,color_FFFFFF,t_70" alt="image"></p>
<h3 id="RDD-算子操作分类"><a href="#RDD-算子操作分类" class="headerlink" title="RDD 算子操作分类"></a>RDD 算子操作分类</h3><ul>
<li><ol>
<li>transformation（转换）</li>
</ol>
<ul>
<li>它可以把一份RDD转换生成一个心的Rdd，延迟加载，不会立即触发任务的真正运行</li>
<li>比如flatMap/map/reduceByKey</li>
</ul>
</li>
<li><ol start="2">
<li>action(动作)</li>
</ol>
<ul>
<li>会触发任务的真正运行</li>
<li>比如collect/SaveAsTextFile</li>
</ul>
</li>
</ul>
<h3 id="RDD的依赖关系"><a href="#RDD的依赖关系" class="headerlink" title="RDD的依赖关系"></a>RDD的依赖关系</h3><p><img src="https://img-blog.csdnimg.cn/20200227121033678.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI4MjU4OTAz,size_16,color_FFFFFF,t_70" alt="image"></p>
<ul>
<li><ol>
<li>narrow dependencies 窄依赖</li>
</ol>
<ul>
<li>where each partition of the parent RDD is used by at most one partition of the child RDD :父RDD的每一个分区至多只被子RDD的一个分区使用</li>
<li>如map/filter/flaMap</li>
<li>不会产生shuffle</li>
</ul>
</li>
<li><ol start="2">
<li>wide dependencies 宽依赖</li>
</ol>
<ul>
<li>where multiple child partitions may depend on it：子RDD的多个分区会依赖于父RDD的同一个分区</li>
<li>比如reduceByKey</li>
<li>会产生shuffle</li>
</ul>
</li>
<li><ol start="3">
<li>lineage 血统</li>
</ol>
<ul>
<li>RDD的产生路径</li>
<li>有向无环图，后期如果某个RDD的分区数据丢失，可以通过lineage重新计算恢复。</li>
</ul>
</li>
</ul>
<h3 id="RDD的缓存机制"><a href="#RDD的缓存机制" class="headerlink" title="RDD的缓存机制"></a>RDD的缓存机制</h3><p>可以把RDD的数据缓存在内存或者磁盘中，后期需要时从缓存中取出，不用重新计算。</p>
<ul>
<li>可以设置不同的缓存级别，如DISK_ONLY,DISK_ONLY_2,MEMORY_AND_DISK_SER_2（内存和磁盘都保存两份并序列化）</li>
<li>对计算复杂的RDD设置缓存。</li>
</ul>
<h3 id="DAG的构建和构建stage"><a href="#DAG的构建和构建stage" class="headerlink" title="DAG的构建和构建stage"></a>DAG的构建和构建stage</h3><ul>
<li>lineage<ul>
<li>它是按照RDD之间的依赖生成的有向无环图</li>
</ul>
</li>
<li>stage<ul>
<li>后期会根据DAG划分stage：从图的lowest节点往前，构建初始stage，往前遍历DAG，如果是窄依赖，则加入此stage，如果是宽依赖则构建新的stage。</li>
<li>stage也会产生依赖：前面stage中task差生的数据流入后面stage中的task去。</li>
<li>划分stage的原因<ul>
<li>由于一个job任务中可能会有大量的宽依赖，由于宽依赖不会产生shufflw，宽依赖会产生shuffle。划分完stage后，在同一个stage中只有窄依赖，则可以对应task并行执行： 所有的stage中由并行执行的task组成。</li>
</ul>
</li>
</ul>
</li>
<li>App，job，Stage，Task之间的关系：<ul>
<li>application 是spark的一个应用程序，包含了客户端写好的代码以及任务运行时所需要的资源信息。后期一个app中有多个action操作，每个action对应一个job，一个job由产生了多个stage，每一个stage内部有很多并行运行的task构成的集合。</li>
</ul>
</li>
</ul>
<h2 id="ref"><a href="#ref" class="headerlink" title="ref"></a>ref</h2><ul>
<li><a href="https://www.jianshu.com/p/7ea0a450eec8" target="_blank" rel="noopener">Scala 中 _ 代表什么</a></li>
<li><a href="https://www.bilibili.com/video/av76914891?from=search&seid=13134650878163258083" target="_blank" rel="noopener">原理视频教程</a></li>
<li><a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala" target="_blank" rel="noopener">RDD源代码定义</a></li>
<li><a href="https://www.runoob.com/scala/scala-tutorial.html" target="_blank" rel="noopener">scala教程</a></li>
<li><a href="">6.824讲义</a></li>
<li><a href="">spark论文原文</a></li>
<li><a href="https://blog.csdn.net/someInNeed/article/details/90047624" target="_blank" rel="noopener">Scala中的”- &gt;”和” -“以及”=&gt;”</a></li>
<li><a href="https://www.zhihu.com/answer/41608400" target="_blank" rel="noopener">从Hadoop进化到Spark</a></li>
<li><a href="http://dblab.xmu.edu.cn/blog/spark/" target="_blank" rel="noopener">子雨大数据之Spark入门教程（Scala版）</a><h2 id="spark实践"><a href="#spark实践" class="headerlink" title="spark实践"></a>spark实践</h2><h3 id="docker安装方式："><a href="#docker安装方式：" class="headerlink" title="docker安装方式："></a>docker安装方式：</h3></li>
<li><a href="https://blog.csdn.net/u013705066/article/details/80030732" target="_blank" rel="noopener">https://blog.csdn.net/u013705066/article/details/80030732</a></li>
</ul>
<h2 id="else"><a href="#else" class="headerlink" title="else"></a>else</h2><h3 id="spark内存计算"><a href="#spark内存计算" class="headerlink" title="spark内存计算"></a>spark内存计算</h3><ul>
<li><a href="https://www.zhihu.com/question/23079001/answer/23569986" target="_blank" rel="noopener">https://www.zhihu.com/question/23079001/answer/23569986</a></li>
</ul>
]]></content>
      <categories>
        <category>distributed computation</category>
      </categories>
      <tags>
        <tag>system</tag>
        <tag>distribute</tag>
        <tag>fault tolerance</tag>
        <tag>big date</tag>
      </tags>
  </entry>
  <entry>
    <title>TLA+资料整理</title>
    <url>/2020/02/24/TLA+%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<ul>
<li><a href="https://github.com/tlaplus" target="_blank" rel="noopener">Github官方库</a></li>
<li><a href="https://blog.csdn.net/solotzg/article/details/80669924" target="_blank" rel="noopener">Raft TLA+形式化验证</a></li>
<li><a href="http://lamport.azurewebsites.net/video/videos.html" target="_blank" rel="noopener">lamport老爷子的官方视频入门教程</a></li>
<li><a href="http://www.tlaplus.net/projects/tlaps-a-tla-proof-system/" target="_blank" rel="noopener">TLA+Proof:TLAPS: A TLA+ Proof System</a></li>
<li><a href="https://www.zhihu.com/people/feng-zun-bao-4/posts" target="_blank" rel="noopener">冯遵宝知乎TLA+教程</a></li>
<li><a href="http://lamport.azurewebsites.net/tla/tla.html" target="_blank" rel="noopener">TLA+官网</a></li>
<li><a href="https://www.jianshu.com/p/b21668967eb5" target="_blank" rel="noopener">siddontang简书TLA+学习笔记</a></li>
<li><a href="https://wenku.baidu.com/u/hitman_007?from=wenku" target="_blank" rel="noopener">离散数学百度文库</a></li>
<li>推荐参考书籍<a href="https://lamport.azurewebsites.net/tla/book.html?back-link=learning.html#book" target="_blank" rel="noopener">Specifying Systems</a></li>
<li><a href="https://lamport.azurewebsites.net/tla/toolbox.html" target="_blank" rel="noopener">IDE工具</a></li>
</ul>
]]></content>
      <categories>
        <category>os</category>
      </categories>
      <tags>
        <tag>system</tag>
        <tag>distribute</tag>
        <tag>fault tolerance</tag>
        <tag>raft</tag>
        <tag>paxos</tag>
      </tags>
  </entry>
  <entry>
    <title>ElasticDL</title>
    <url>/2020/02/24/ElasticDL-%20A%20Kubernetes-native%20Deep%20Learning%20Framework/</url>
    <content><![CDATA[<h1 id="k8s原生的深度学习框架"><a href="#k8s原生的深度学习框架" class="headerlink" title="k8s原生的深度学习框架"></a>k8s原生的深度学习框架</h1><p>ElasticDL是一个kubernets原生的深度学习框架，构建在TensorFlow 2.0之上，支持容错和弹性调度。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>TensorFlow有其本地的分布式计算特性，可以进行故障恢复。当某些进程失败时，分布式计算作业也会失败;但是，我们可以重新启动作业，并从最近的检查点文件恢复其运行时状态。</p>
<p>ElasticDL增强了TensorFlow的分布式训练特性，支持容错。在某些进程失败的情况下，作业将继续运行。因此，ElasticDL不需要检查点，也不需要从检查点恢复。</p>
<p>容错特性使得ElasticDL与Kubernetes基于优先级的抢占机制协同工作，实现弹性调度。当Kubernetes终止一个作业的某些进程，将资源释放给具有更高优先级的新作业时，当前作业不会失败，而是在资源更少的情况下继续工作。</p>
<p>弹性调度可以显著提高集群的整体利用率。假设一个集群有N个gpu，一个作业使用其中一个。如果没有弹性调度，一个使用N个gpu的新作业在开始之前必须等待第一个作业完成。这个等待时间可能是几个小时、几天甚至几周。在这段很长的时间内，集群的利用率是1/N。使用弹性调度，新作业可以在N-1个GPU上立即开始运行，Kubernetes可以在第一个作业完成后将其GPU消耗增加1。在本例中，总利用率为100。</p>
<p>ElasticDL的弹性调度特性来自于它的Kubernetes本机设计——它不依赖Kubeflow等Kubernetes扩展来运行TensorFlow程序;相反，ElasticDL作业的主进程调用Kubernetes API来启动worker和参数服务器;它还监视诸如进程/pod杀死之类的事件，并对此类事件作出响应，以实现容错。</p>
<p>总之，在Kubernetes集群的情况下，ElasticDL通过容错和弹性调度增强了TensorFlow。我们提供了一个教程，演示如何在谷歌云中设置Kubernetes集群并在那里运行ElasticDL作业。我们尊重TensorFlow的本地分布式计算特性，它不需要像Kubernetes这样的特定计算平台，并且允许TensorFlow在任何平台上运行。</p>
<h2 id="主要特性"><a href="#主要特性" class="headerlink" title="主要特性"></a>主要特性</h2><h3 id="弹性调度和容错"><a href="#弹性调度和容错" class="headerlink" title="弹性调度和容错"></a>弹性调度和容错</h3><ul>
<li>ElasticDL通过Kubernetes-native设计实现容错，并与Kubernetes基于优先级的抢占机制协同工作，实现深度学习任务的弹性调度。</li>
</ul>
<h3 id="TF2-0：Eager-Execution"><a href="#TF2-0：Eager-Execution" class="headerlink" title="TF2.0：Eager Execution"></a>TF2.0：Eager Execution</h3><ul>
<li>分布式深度学习框架在模型更新之前需要了解局部梯度。Eager Execution允许ElasticDL在不侵入图形执行过程的情况下执行。</li>
</ul>
<h3 id="极简API"><a href="#极简API" class="headerlink" title="极简API"></a>极简API</h3><p>给定一个用Keras API定义的模型，用命令行训练该模型。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">elasticdl train --model_def&#x3D;mnist_functional_api.custom_model --training_data&#x3D;&#x2F;mnist&#x2F;train --output&#x3D;output</span><br></pre></td></tr></table></figure>

<h3 id="与SQLFlow整合"><a href="#与SQLFlow整合" class="headerlink" title="与SQLFlow整合"></a>与SQLFlow整合</h3><ul>
<li>ElasticDL将与SQLFlow无缝集成，将SQL与ElasticDL连接到分布式深度学习任务。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT * FROM employee LABEL income INTO my_elasticdl_model</span><br></pre></td></tr></table></figure>


</li>
</ul>
<h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><ul>
<li><a href="https://github.com/sql-machine-learning/elasticdl" target="_blank" rel="noopener">github主页</a></li>
</ul>
]]></content>
      <categories>
        <category>distributed computation</category>
      </categories>
      <tags>
        <tag>system</tag>
        <tag>distribute</tag>
        <tag>ml</tag>
        <tag>fault tolerance</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/02/24/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
      <categories>
        <category>BlogTest</category>
      </categories>
      <tags>
        <tag>BlogTest</tag>
      </tags>
  </entry>
</search>
